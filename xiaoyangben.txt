小样本学习通常有zero-shot-learning和one-shot-learning等等，区别在于是否有标记，而few-shot-learning通常是有一小部分有标记样本，大部分无标记样本，我们主要讨论小样本学习思路：
通常由如下三种思路，但也不是绝对的分开的
1，fine-tuning技术
该技术通常是在一个大数据集上面训练网络，然后将其在小数据集上进行微调

2，Mode Based
该方法主要是有传统的存储增强型小样本学习，增加了存储记忆网络，使小样本网络训练时将长时间权重缓慢更新与短时间存储记忆快速恢复学习相结合，代表论文有《 One-shot learning with memory-augmented neural networks》

Meta Network 的快速泛化能力源自其“快速权重”的机制，在训练过程中产生的梯度被用来作为快速权重的生成。模型包含一个 meta learner 和一个 base learner，meta learner 用于学习 meta task 之间的泛化信息，并使用 memory 机制保存这种信息，base learner 用于快速适应新的 task，并和meta learner 交互产生预测输出。该论文涉及到元学习相关内容，meta learner用于快速更新，而base learner用于慢速更新。

3，Metri Based
该方法主要是通过训练样本和测试样本间的相似性度量来判断测试样本属于某个类别
主要有孪生网络《Siamese Neural Networks》通过将不同的输入分别送入两个相同的网络然后通过预测是否是同一类作为损失函数，进行更新。训练结束后，将待预测的样本与训练集同时送入，最后概率最大的即为测试样本所代表的类。

匹配网络《Matching Network》学习方法有点像孪生网络，但不完全相同，本文章基于传统机器学习的一个原则， 即测试应该和训练实在同意条件下进行，所以训练的时候每次看到一个新的类别的小量样本，这和测试阶段是一样的。用到了存储的思想以及注意力机制，主要是采用了两个嵌入函数，将训练集和测试集进行embedded操作，然后计算嵌入后样本间的cosine，以此作为度量标准。当然，对于训练集的嵌入层参数更新只依赖于训练集，但是测试集的嵌入层更新则会受到训练集嵌入层的影响。

原型网络《Prototypical Networks》则是直接将原始的训练集样本进行嵌入后聚类，将分类问题看成聚类和近邻问题，然后将测试样本嵌入后归类到最相近的类别。  文章采用在Bregman散度下的指数族分布的混合密度估计，实验表明squared Euclidean距离比cosine距离要好14到17个点。

相关网络《Relation Network》则是通过学习一个度量器来对样本之间的关系远近进行度量，以前的度量方法都是固定的采取某种距离度量方法，这样对模型的效果并不好，会限制模型泛化能力，因此通过这种方法可以自己学习到一个度量关系的度量器。在测试阶段的时候，则只是一个测试样本与他们之间的关系进行度量，然后会输出可能类别概率及分布，求出对应类别。

4，meta learning
meta learning是机器学习的一个子领域，它自动学习一些应用于机器学习实验的元数据，主要目的是使用这些元数据来自动学习如何在解决不同类型的学习问题时变得灵活，从而提高现有的学习算法。灵活性是非常重要的，因为每个学习算法都是基于一组有关数据的假设，即它是归纳偏(bias)的。这意味着如果bias与学习问题中的数据相匹配，那么学习就会很好。学习算法在一个学习问题上表现得非常好，但在下一个学习问题上表现得非常糟糕。这对机器学习或数据挖掘技术的使用造成了很大的限制，因为学习问题与不同学习算法的有效性之间的关系尚不清楚。

递归记忆模型《Memory-Augmented Neural Networks》
文章基于神经网络图灵机（NTMs）的思想，因为NTMs能通过外部存储（external memory）进行短时记忆，并能通过缓慢权值更新来进行长时记忆，NTMs可以学习将表达存入记忆的策略，并如何用这些表达来进行预测。由此，文章方法可以快速准确地预测那些只出现过一次的数据。文章基于LSTM等RNN的模型，将数据看成序列来训练，在测试时输入新的类的样本进行分类。具体地，网络的输入把上一次的y (label)也作为输入，并且添加了external memory存储上一次的x输入，这使得下一次输入后进行反向传播时，可以让y (label)和x建立联系，使得之后的x能够通过外部记忆获取相关图像进行比对来实现更好的预测。这里的RNN就是meta-learner。

优化器学习 《meta-learning LSTM》（Optimization as a model for few-shot learning）
文章学习的是一个模新参数的更新函数或更新规则。它不是在多轮的episodes学习一个单模型，而是在每个episode学习特定的模型。具体地，学习基于梯度下降的参数更新算法，采用LSTM表达meta learner，用其状态表达目标分类器的参数的更新，最终学会如何在新的分类任务上，对分类器网络(learner)进行初始化和参数更新。这个优化算法同时考虑一个任务的短时知识和跨多个任务的长时知识。文章设定目标为通过少量的迭代步骤捕获优化算法的泛化能力，由此meta learner可以训练让learner在每个任务上收敛到一个好的解。另外，通过捕获所有任务之前共享的基础知识，进而更好地初始化learner。
具体地，学习基于梯度下降的参数更新算法，采用 LSTM 表达 meta learner，用其状态表达目标分类器的参数的更新，最终学会如何在新的分类任务上，对分类器网络（learner）进行初始化和参数更新。这个优化算法同时考虑一个任务的短时知识和跨多个任务的长时知识。

模型无关自适应《Model-Agnostic》
该论文核心思想是将训练过程划分为多个阶段，每个阶段进行一个小的训练，将其看成元学习机制。核心思想是使模型有一个好的初始化权值，能够迅速fine-tuning从而达到一个收敛值。文章提出的方法，可以学习任意标准模型的参数，并让该模型能快速适配。方法认为，一些中间表达更加适合迁移，比如神经网络的内部特征。因此面向泛化性的表达是有益的。因为我们会基于梯度下降策略在新的任务上进行finetune，所以目标是学习这样一个模型，它能对新的任务从之前任务上快速地进行梯度下降，而不会过拟合。事实上，是要找到一些对任务变化敏感的参数，使得当改变梯度方向，小的参数改动也会产生较大的loss。